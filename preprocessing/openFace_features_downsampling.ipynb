{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7de5e5b2",
   "metadata": {},
   "source": [
    "## This notebook performs downsampling on the OpenFace Extracted features' dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3396492",
   "metadata": {},
   "source": [
    "1. Downsampling - to a specified ```sampling_size```\n",
    "2. Preprocesses the downsampled data\n",
    "    - Excludes datapoints in ```failure videos``` which are before the ```failureOccurrence_timestamp```\n",
    "3. Merges all ```participants``` all ```videos``` that are downsampled to a particular ```sampling_size``` and preprocessed (as per #2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eff8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e06fcb",
   "metadata": {},
   "source": [
    "### Specify paths\n",
    "\n",
    "- ```downsampling_directory``` - path to the directory where the downsampled ```df's``` are stored for each ```participant``` \n",
    "    - the directories are then created based on the convention of ```/{frequency}fps_downsampling/```  \n",
    "\n",
    "```participant_features_directory``` - path where the original openFace extracted features of the participants are stored locally  \n",
    "```participant_database``` - stores the list of all the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea75a85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now, open openface_features files for each participant\n",
    "files_to_ignore = ['.DS_Store', '5702']\n",
    "participant_features_directory = '../../studyData/openface_datasets/facial_features_openface_cut_dataset_1s/'\n",
    "participant_database = [participant_folder for participant_folder in os.listdir(participant_features_directory) if participant_folder not in files_to_ignore]\n",
    "len(participant_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed111532",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "\n",
    "\n",
    "- ```required_features``` - list of all the features that are to be considered and downsampled\n",
    "- ```final_features``` - list of ```metadata columns + required_features```\n",
    "- ```participant_folder``` - contains the list of all the extracted features dataframe of the ```response_video```s of the ```participant```\n",
    "- ```csv_file``` - openface extracted feature files of the ```response_video``` of a ```participant```\n",
    "- ```target_class``` - OHE value of the class the ```response_video``` belongs to.\n",
    "- ```required_df``` - a dataframe that contains all the ```final_features``` from the original ```df``` dataframe\n",
    "\n",
    "\n",
    "- ```sampling_size``` - indicates how many ```datapoints in a second``` is to be considered and retained\n",
    "- ```frequency = 1 / sampling_size```\n",
    "- ```last_timestamp``` - last recorded time of final extracted frame of the participant's ```response_video``` in seconds\n",
    "- ```valid_timestamps``` - list of all the ```sampling_size``` intervals\n",
    "- ```downsampled_df``` - final ```df``` that contains downsampled data with ```final_features``` values\n",
    "- ```(from_time, time]``` - time range of the datapoints based on the original_df that is to be considered for the window\n",
    "- ```dataPoints_within_window``` - dataPoints within the timerange to be considered for downsampling to a single datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f7d1dc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the sampling size: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Participant ID = 9941: 100%|████████████████████| 29/29 [00:24<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Frames: \n",
      "Class 0: 463\n",
      "Class 1: 402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Filter out the specific warning category\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "required_features = ['timestamp', 'gaze_0_x','gaze_0_y','gaze_0_z','gaze_1_x','gaze_1_y','gaze_1_z','gaze_angle_x','gaze_angle_y','pose_Tx', 'pose_Ty', 'pose_Tz','pose_Rx', 'pose_Ry', 'pose_Rz','AU01_r','AU02_r','AU04_r','AU05_r','AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r','AU15_r','AU17_r','AU20_r','AU23_r','AU25_r','AU26_r','AU45_r','AU01_c','AU02_c','AU04_c','AU05_c','AU06_c','AU07_c','AU09_c','AU10_c','AU12_c','AU14_c','AU15_c','AU17_c','AU20_c','AU23_c','AU25_c','AU26_c','AU28_c','AU45_c']\n",
    "# Some initial information regarding the participant and their corresponding responseVideo information to be included\n",
    "final_features = ['participant_id', 'response_video', 'class']\n",
    "\n",
    "sampling_size = float(input('Enter the sampling size: '))\n",
    "# round it to the nearest Xth second (frequency = 1/sampling_size)\n",
    "frequency = int(1 / sampling_size)\n",
    "\n",
    "video_clipping_duration = 1\n",
    "downsampling_directory = f\"../../studyData/openface_datasets/downsampled_feature_data/\" + f\"facial_features_openface_cut_dataset_{video_clipping_duration}s_{frequency}fps_downsampling/\"\n",
    "\n",
    "if not os.path.exists(downsampling_directory):\n",
    "    os.makedirs(downsampling_directory)\n",
    "\n",
    "with tqdm(total=len(participant_database)) as pbar:\n",
    "    class_0 = 0\n",
    "    class_1 = 0\n",
    "    for participant in sorted(participant_database):\n",
    "        total_participant_frames = 0\n",
    "        num_data_points_below_confidence_score = 0\n",
    "\n",
    "        participant_folder_path = f'{participant_features_directory}{participant}/'\n",
    "        participant_folder = os.listdir(participant_folder_path)\n",
    "        for csv_file in sorted(participant_folder):\n",
    "\n",
    "            if csv_file in files_to_ignore or \"warmup\" in csv_file:\n",
    "                continue\n",
    "\n",
    "            # Identify the class to which the csv_file belongs to\n",
    "            target_class = int(csv_file.split(\".\")[0].split(\"_\")[-1])\n",
    "            responseVideo_name = csv_file.split(\"_\")[0] + \"_\" + csv_file.split(\"_\")[1]\n",
    "\n",
    "            # Read the features csv file\n",
    "            csv_file_path = f'{participant_folder_path}{csv_file}'\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            \n",
    "#             ## Check number of datapoints below confidence level\n",
    "#             num_data_points_below_confidence_score += (df['confidence'] < 0.80).sum()\n",
    "#             total_participant_frames += df.shape[0]\n",
    "            \n",
    "            ## Check the number of datapoint (frame) classes\n",
    "            if target_class == 0:\n",
    "                class_0 += 1\n",
    "            else:\n",
    "                class_1 += 1\n",
    "            \n",
    "            # Create a new dataframe - 'required_df' that retains all the feature columns that are required from the original dataframe - 'df'\n",
    "            required_df = df[required_features].copy()\n",
    "\n",
    "            # add participant column\n",
    "            required_df['participant_id'] = participant\n",
    "            required_df['class'] = target_class\n",
    "            required_df['response_video'] = responseVideo_name\n",
    "\n",
    "            # Reorganize the column order\n",
    "            required_df = required_df[final_features + required_features]\n",
    "\n",
    "            ## TODO: Downsampling\n",
    "\n",
    "            # Obtain the last_timestamp for every response_video data\n",
    "            last_timestamp = required_df['timestamp'].values[-1] # NOTE: this is in seconds\n",
    "            last_timestamp = round(last_timestamp * frequency) / frequency\n",
    "            # Now we get a list of timestamps once every 'sampling_size' seconds, until the last timestamp\n",
    "            valid_timestamps = np.arange(0, last_timestamp + sampling_size, sampling_size)\n",
    "\n",
    "            # A new dataframe to store the downsampled datapoints from the 'required_df'\n",
    "            downsampled_df = pd.DataFrame()\n",
    "\n",
    "            for time in valid_timestamps:\n",
    "                if time == valid_timestamps[0]:\n",
    "                    # Retrieve the rows between the range (from_time, time]\n",
    "                    downsampled_df = pd.concat([downsampled_df, required_df[required_df['timestamp'] <= time]])\n",
    "                    from_time = time\n",
    "                else:\n",
    "                    dataPoints_within_window = required_df[(required_df['timestamp'] > from_time) & (required_df['timestamp'] <= time)]\n",
    "\n",
    "                    # Check if there exists any datapoints within the range specified: (from_time, time]\n",
    "                    if len(dataPoints_within_window) > 0:\n",
    "\n",
    "                        # Now that you have datapoints within the specified window range: (from_time, time]\n",
    "                        # Calculate some statistics on the feature columns present in the dataframe\n",
    "                        # i.e: mean(), max(), etc.. on the respective feature columns\n",
    "\n",
    "                        # NOTE: here we reduce the range of dataPoints to a single dataPoint after calculating the statistics\n",
    "                        for column in dataPoints_within_window.columns:\n",
    "\n",
    "                            # for the feature columns that contain values based on classification\n",
    "                            # i.e: AU##_c :- columns, find the max value amongst them (i.e: 0 or 1)\n",
    "                            if '_c' in column:\n",
    "                                dataPoints_within_window[column] = dataPoints_within_window[column].max()\n",
    "                            # for all other columns except few, calculate the aggregate value\n",
    "                            elif column not in ['participant_id', 'class', 'response_video', 'timestamp']:\n",
    "                                dataPoints_within_window[column] = dataPoints_within_window[column].mean()\n",
    "                            elif column == 'timestamp':\n",
    "                                dataPoints_within_window[column] = time\n",
    "\n",
    "                        # Now we add the dataPoints_within_window :- that have been reduced to a single datapoint\n",
    "                        # to the final - downsampled_df\n",
    "                        downsampled_df = pd.concat([downsampled_df.T, dataPoints_within_window.iloc[0, :]], axis = 1).T\n",
    "                    else:\n",
    "                        print(f'No datapoints within the specified range! ({from_time}, {time}]')\n",
    "\n",
    "                    # Update the range of the time :- to shift the window\n",
    "                    from_time = time\n",
    "                # print(downsampled_df.shape)\n",
    "            \n",
    "            #make new folder in data/openFace_features_data directory, called {frequency}fps_downsampling\n",
    "            downsampling_directory = downsampling_directory\n",
    "            if not os.path.exists(downsampling_directory):\n",
    "                os.mkdir(downsampling_directory)\n",
    "            \n",
    "            # If the participant directory does not exist for the downsampling, create a directory\n",
    "            if not os.path.exists(f'./{downsampling_directory}/{participant}/'):\n",
    "                os.mkdir(f'./{downsampling_directory}/{participant}/')\n",
    "\n",
    "            # Save the downsampled features file as - ch1_5fps.csv in the directory: participant\n",
    "            downsampled_df.to_csv(f'./{downsampling_directory}/{participant}/{responseVideo_name}_{frequency}fps.csv', index = False)\n",
    "        \n",
    "#         num_datapoints_below_confidence_percentage = num_data_points_below_confidence_score / total_participant_frames * 100\n",
    "#         print(f\"Participant {participant}: Datapoints below 80% confidence level = {num_datapoints_below_confidence_percentage:.03f}% ({num_data_points_below_confidence_score} / {total_participant_frames})\")\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f'Participant ID = {participant}')\n",
    "    print(\"Number of Frames: \")\n",
    "    print(f\"Class 0: {class_0}\\nClass 1: {class_1}\")\n",
    "\n",
    "# After you're done with the code, you can reset the warning filters\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d111fb0",
   "metadata": {},
   "source": [
    "#### Merge\n",
    "\n",
    "- Here we merge - all ```participants``` all ```responseVideos``` into a single dataframe called as ```allParticipants_frequencyfps_downsampled_preprocessed``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "046370f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "downsampled_participant_database = os.listdir(downsampling_directory)\n",
    "downsampled_participant_database = [participant_folder for participant_folder in downsampled_participant_database if participant_folder not in files_to_ignore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cee6928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Participant ID = 9941: 100%|████████████████████| 29/29 [00:01<00:00, 27.08it/s]\n"
     ]
    }
   ],
   "source": [
    "allParticipants_df = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(downsampled_participant_database)) as pbar:\n",
    "    for participant in sorted(downsampled_participant_database):\n",
    "        participant_folder_path = f'{downsampling_directory}{participant}/'\n",
    "        participant_folder = os.listdir(participant_folder_path)\n",
    "\n",
    "        for csv_file in sorted(participant_folder):\n",
    "            if csv_file in files_to_ignore:\n",
    "                continue\n",
    "            current_df = pd.read_csv(f'{participant_folder_path}{csv_file}')\n",
    "            allParticipants_df = pd.concat([allParticipants_df, current_df])\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f'Participant ID = {participant}')\n",
    "allParticipants_df.to_csv(f'{downsampling_directory}allParticipants_{frequency}fps_downsampled_preprocessed.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e26531",
   "metadata": {},
   "source": [
    "#### Here, we normalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e3ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "allParticipants_df_norm = copy.deepcopy(allParticipants_df)\n",
    "\n",
    "columns_to_normalize = allParticipants_df_norm.columns[4:]\n",
    "scaler = StandardScaler()\n",
    "allParticipants_df_norm[columns_to_normalize] = scaler.fit_transform(allParticipants_df_norm[columns_to_normalize])\n",
    "\n",
    "allParticipants_df_norm.to_csv(f'{downsampling_directory}/allParticipants_{frequency}fps_downsampled_preprocessed_norm.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e673c104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
